## Logistic Regression & Neural NWs

A logistic regression (also a neural network with one node) looks like the figure.

<img src="/static/media/NNBook/imgs/lrn11.png"  style="width: 3in">


The inputs are passed in and through back propagation, W is adjusted and refined to find the optimal values for W and b to minimize the cost function.
<img src="/static/media/NNBook/imgs/lrn2.png"  style="width: 3in">

A Neural Network may consists of more than one such node stacked, also possibly arranged in layers to create a complex network. Each layer will compute z and a and finally last layer will compute the loss function â„’.
<img src="/static/media/NNBook/imgs/lrn3.png"  style="width: 3in">


A various types of Neural Network. A two layer network is shown. Two because, most times, input layer is not counted as a layer.
<img src="/static/media/NNBook/imgs/lrn4.png"  style="width: 3in">


### Computations of weights in NN.
Similar to logistic regression in forward propagation, for each cell we compute the activations Z1, Z2 etc. Think each cell as one logistic regression cell. The back propagation happens for each cells very similar to one cell logistic regression and it happens to every cell to back propagate.

<img src="/static/media/NNBook/imgs/lrn5.png"  style="width: 3in">


### Activation Functions.

Some activation functions are:
* Sigmoid, 
* tanh, 
* RELU, 
* Leaky RELU

##### Sigmoid Activation function
Sigmoid function that is shown that we have been using so far

<img src="/static/media/NNBook/imgs/ac1.png"  style="width: 3in">


##### tanh function

<img src="/static/media/NNBook/imgs/act2.png"  style="width: 6in">


##### RELU functions or Leaky RELU function
<img src="/static/media/NNBook/imgs/act3.png"  style="width: 6in">


---
#### Take aways.
>* Never use sigmoid in hidden layers except output layers
*  If you are not sure what to use as activation for hidden layer, choose RELU
* Do not use Linear Activation function in hidden layer - it practically makes layers useless.
* Only place where linear activation function may be used is in output layer 
