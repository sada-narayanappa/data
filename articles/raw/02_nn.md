## Rationale for Logistic Regression loss function.

$\hat{y}  = \sigma(\theta^TX + b)$ 

We want to interpret the predicted value:

$$
\widehat{y} = P(Y = 1|X):
\begin{bmatrix} 
\begin{align} 
\text{if   } y &= 1 \text{ then }  P(y=1|X) =  \hat{y} \\\  
\text{if   } y &= 0 \text{ then }  P(y=0|X) =  1 - \hat{y} \\\
\end{align} 
\end{bmatrix} 
$$ 

We can combine both cases in one equation:

$$ 
P(Y|X) = \hat{y}^y(1 − \hat{y})^{(1 − y)} 
$$

when $y$ is 0, we want the predicted value to represent $P(Y|X) = (1 − \hat{y})$; similarly other case $y$ is 1, we want predictd value $\hat{y}$ to be $P(Y|X) = \hat{y}$

We want to maximize this cost across all examples:  $P = \overset{m}{\prod_{i = 1}}P(y^i|x^i)$

Maximizing the probability is equivalent to maximizing the log of the probability. Therefore,
 
$$
log(P_{\text{all\ training\ examples}}) = 
\log\left( 
\overset{m}{\prod_i} P(y^{i}|X^{i})
\right) = 
\overset{m}{\sum_i}\log(P(y^i|X^i))
$$

$\log (P(y|x)) = y^i\log (\hat{y}^i) + (1 − y^i)\log (1−\hat{y}^i)$


We want to maximize P and minimize the loss that is -ve of P, therefore, we want to choose parameters that minimizes the cost of the function that can be does using maximum likelihood estimation methods. Therefore the final cost function from 2.3.2 averaged over all training examples is:

$J(\theta) = - \frac{1}{m}\overset{m}{\sum_{i = 1}}y^{i}\log({\widehat{y}}^{i}) + (1 - y^{i})\log(1 - {\widehat{y}}^{i}))$

