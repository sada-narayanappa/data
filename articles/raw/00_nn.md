# Detailed Derivation of Back Propogation

Now, we show detailed derivation of equation of derivate of a loss function.

$$\begin{equation} 
J(\theta) = - \frac{1}{m}\overset{m}{\sum_{i = 1}}y^{i}\log({\widehat{y}}^{i}) + (1 - y^{i})\log(1 - {\widehat{y}}^i)) \\\
\end{equation} 
$$   

$$ 
\theta x^i = \theta_{0} + \theta_1 x_1^i + \ldots + \theta_p x_p^i
$$

Consider from (1),

$$\begin{equation} 
\log{\widehat{y}}^{i} = \log\frac{1}{1 + e^{- \theta x^{i}}} = - \log(1 + e^{- \theta x^{i}})
\end{equation} 
$$

$$\begin{equation} 
\log(1 - \widehat{y}^{i}) = \log(1 - \frac{1}{1 + e^{- \theta x^{i}}}) = \log(e^{- \theta x^{i}}) - \log(1 + e^{- \theta x^{i}}) = - \theta x^{i} - \log(1 + e^{- \theta x^{i}}),
\end{equation} 
$$
> we used: log (x/y) = log (x) − log (y)

Since our original cost function is the form of:

$$ J(\theta) = - \frac{1}{m}\overset{m}{\sum_{i = 1}}y^{i}\log({\widehat{y}}^{i}) + (1 - y^{i})\log(1 - {\widehat{y}}^{i})) $$

Plugging in the two simplified expressions above, we obtain

$$J(\theta) = - \frac{1}{m}\overset{m}{\sum_{i = 1}}\left\lbrack - y^{i}(\log(1 + e^{- \theta x^{i}})) + (1 - y^{i})( - \theta x^{i} - \log(1 + e^{- \theta x^{i}})) \right\rbrack$$

which can be simplified to, substituting (2) & (3):

$$\begin{equation} 
J(\theta) = - \frac{1}{m}\overset{m}{\sum_{i = 1}}
\left\lbrack 
y^i \theta x^i - \theta x^i - \log(1 + e^{-\theta x^i}) 
\right\rbrack = - \frac{1}{m}\overset{m}{\sum_i} 
\left\lbrack 
 y^i\theta x^{i} - \log(1 + e^{\theta x^{i}})
\right\rbrack
\end{equation}
$$ 
 
where the second equality follows from

$$
−θxi − \log(1+e^{−\theta xi}) =  −[\log e^{θxi} + \log(1+e^{−\theta xi})] =  −\log(1 + e^{θxi} ).
$$

> we used $\log(x) + log(y) = \log(xy)$
 
All needed now is to compute the partial derivatives of (4) w.r.t. $\theta_j$.

As

$$
\frac{\partial}{\partial\theta_j}J(\theta) = 
\frac{\partial}{\partial\theta_j}
\left\lbrack -\frac{1}{m}\overset{m}{\sum_i}
\left\lbrack 
 y^i\theta x^{i} - \log(1 + e^{\theta x^{i}})
\right\rbrack 
\right\rbrack 
$$ 
 
$$\begin{equation} 
\frac{\partial}{\partial\theta_{j}}y^i\theta x^{i} = y^i x_j^i
\end{equation}
$$

$$\begin{equation} 
\frac{\partial}{\partial\theta_j} \log(1 + e^{\theta x^i}) = \frac{x_j^i e^{\theta x^i}}{1 + e^{\theta x^i}}
= x_j^i  . \widehat{y}^i 
\end{equation}
$$
 
Thus,

>$$
\frac{\partial}{\partial\theta_j}J(\theta) = 
\overset{m}{\sum_i} 
(\widehat{y}^i - y^i) x_j^i
$$ ◼
 
Once we know the derivative of the Cost function. Next step is to use it to find the parameters, 𝜃 that minimizes the cost function.

