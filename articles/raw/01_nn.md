## Gradient Descent (GD)

We have,

$$\begin{aligned} 
\widehat{y} &= \sigma(\theta^T X + b) \\\
\sigma(z)   &= \frac{1}{1 + e^{-z}}   \\\ 
J(\theta,b) &= -\frac{1}{m}\overset{m}{\sum_i} 
y^i\log({\widehat{y}}^i) + (1 - y^i)\log(1 - \widehat{y}^i) &  \\\
\frac{\partial}{\partial\theta_j}J(\theta) &= \overset{m}{\sum_i} 
\left(
\widehat{y}^{(i)} - y^{(i)}\right) . x_j^i   
\end{aligned} 
$$ 
   

We can use the above equations in Gradient Descent algorithm to find the optimal (ùúÉ, b) to minimize the cost function $J(\theta)$.  The rationale is as follows. Any convex function would have a plot as shown below :

<table width=100% padding=20px  margin=10px ><tr><td>
<img align=left valign=top  src="/static/media/NNBook/imgs/conv1.png"  style="width: 3in">
</td><td>
  <img align=middle valign=top  src="/static/media/NNBook/imgs/conv2.png"  style="width: 3in"> 
</tr></table>
  <br/>
  <br/>

##### Procedure 

1. Initialize $(\theta, b)$ to any random value

2. Compute the derivative as given in 

3. Update $\theta$ as $\theta = \theta - \alpha\overset{\cdot}{}\frac{\partial}{\partial\theta_{j}}J(\theta)$ where $\alpha$ is called as learning rate determines how big of a step we take each time.

4. Continue until the error reaches acceptable limit.

Why does this work. Let‚Äôs take an example as shown in the right side of the figure above. If the point is on the right side, the slope (or the derivative is +ve) therefore we subtract from $\theta$. On the other hand, if it is on the left of the minimum the slope is -ve and we move $\theta$ towards right. Therefore GD will move $\theta$ in the right direction to find minimum point.

